{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-08T05:52:59.385423500Z",
     "start_time": "2024-08-08T05:52:35.457463200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully copied to resources/German_credit_NEW.xlsx\n",
      "['Purpose', 'Sex & Marital Status', 'Length of current employment', 'Instalment per cent', 'Account Balance', 'Payment Status of Previous Credit', 'Value Savings/Stocks', 'Credit Amount', 'Age (years)', 'Duration of Credit (month)', 'Most valuable available asset', 'Concurrent Credits']\n",
      "\n",
      "***************************\n",
      "Best binning number: 54 with AUC: 0.8203571428571428\n",
      "***************************\n",
      "\n",
      "\n",
      "Selected top IV features for modeling:\n",
      " Duration of Credit (month), IV value: 1.0495353305806965\n",
      " Credit Amount, IV value: 1.0314557255592238\n",
      " Account Balance, IV value: 0.6660115033513336\n",
      " Age (years), IV value: 0.6129095040344794\n",
      " Payment Status of Previous Credit, IV value: 0.2932335473908263\n",
      " Value Savings/Stocks, IV value: 0.19600955690422672\n",
      " Purpose, IV value: 0.16919506567307832\n",
      " Most valuable available asset, IV value: 0.11263826240979674\n",
      " Length of current employment, IV value: 0.086433631026641\n",
      " Concurrent Credits, IV value: 0.057614541955647885\n",
      " Sex & Marital Status, IV value: 0.04467067763379073\n",
      "Features used in Model Training: ['Duration of Credit (month)_woe', 'Credit Amount_woe', 'Account Balance_woe', 'Age (years)_woe', 'Payment Status of Previous Credit_woe', 'Value Savings/Stocks_woe', 'Purpose_woe', 'Most valuable available asset_woe', 'Length of current employment_woe', 'Concurrent Credits_woe', 'Sex & Marital Status_woe']\n",
      "Logistic Regression Cross-validated accuracy: 0.7850 ± 0.0300\n",
      "Decision Tree Cross-validated accuracy: 0.6900 ± 0.0318\n",
      "Random Forest Cross-validated accuracy: 0.7533 ± 0.0476\n",
      "Gradient Boosting Cross-validated accuracy: 0.7500 ± 0.0253\n",
      "Support Vector Machine Cross-validated accuracy: 0.7717 ± 0.0327\n",
      "Stacking Classifier Cross-validated accuracy: 0.7850 ± 0.0314\n",
      "\n",
      "Best Model: Logistic Regression\n",
      "Accuracy: 0.8150\n",
      "AUC: 0.8481\n",
      "Confusion Matrix:\n",
      "[[248  32]\n",
      " [ 42  78]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87       280\n",
      "           1       0.71      0.65      0.68       120\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.78      0.77      0.77       400\n",
      "weighted avg       0.81      0.81      0.81       400\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "\n",
    "# Model Selection and Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "#-----------------------------------------------------------------\n",
    "# 读取源Excel文件\n",
    "source_file_path = 'resources/German_credit.xlsx' \n",
    "df = pd.read_excel(source_file_path)\n",
    "# 创建一个新的Excel文件并将数据写入其中\n",
    "new_file_path = 'resources/German_credit_NEW.xlsx'\n",
    "df.to_excel(new_file_path, index=False)\n",
    "print(f'Data successfully copied to {new_file_path}')\n",
    "# data现在是我们用来分析的源数据附件\n",
    "data = pd.read_excel(new_file_path)\n",
    "#-----------------------------------------------------------------\n",
    "# 选择的影响力大的变量数量，前十名----------------------\n",
    "numVar = 10\n",
    "\n",
    "# 绘制数据相关性热图\n",
    "# 找到对target最有影响力的变量，依次排名，选numVar个较有影响力的\n",
    "def find_top_correlated_features(df2, target1, n):\n",
    "    corr_matrix = df2.corr()\n",
    "    target_cor = corr_matrix[target1].abs().sort_values(ascending=False)\n",
    "    top_features = target_cor.index[1:n+1]  # 排除掉target本身\n",
    "    return top_features, target_cor\n",
    "\n",
    "selected_features_CHMap, target_corr = find_top_correlated_features(df, 'target', numVar)\n",
    "\n",
    "# 绘制随机森林模型相关图\n",
    "# 特征选择和目标变量\n",
    "features = data.columns[data.columns != 'target'] \n",
    "target = 'target'\n",
    "# 分割数据集\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "# 构建随机森林模型以评估特征重要性\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "# 获取特征重要性\n",
    "importances = rf.feature_importances_\n",
    "feature_importances = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "# 筛选重要性较高的变量，依次排名，选numVar个较有影响力的\n",
    "selected_features_rf = feature_importances.nlargest(numVar, 'Importance')['Feature']\n",
    "#-----------------------------------------------------------------\n",
    "# 采用了合并并去重采用了两列排名靠前的变量\n",
    "# 合并并去重\n",
    "combined_features = selected_features_rf.tolist() + selected_features_CHMap.tolist()\n",
    "unique_features_set = set(combined_features)\n",
    "selected_features_final = list(unique_features_set)\n",
    "print(selected_features_final)\n",
    "#-----------------------------------------------------------------\n",
    "# 分箱函数\n",
    "def binning(bdata, bcolumn, n_bins, strategy='uniform'): \n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "    bdata[bcolumn + '_binned'] = est.fit_transform(bdata[[bcolumn]])\n",
    "    return bdata, est\n",
    "\n",
    "# 计算WOE, IV函数\n",
    "def calculate_woe_iv(cdata, ccolumn, ctarget):\n",
    "    woe_dict = {}\n",
    "    iv = 0\n",
    "    total_good = sum(cdata[ctarget] == 0)\n",
    "    total_bad = sum(cdata[ctarget] == 1)\n",
    "    \n",
    "    for bin in sorted(cdata[ccolumn].unique()):\n",
    "        bin_data = cdata[cdata[ccolumn] == bin]\n",
    "        good = sum(bin_data[ctarget] == 0)\n",
    "        bad = sum(bin_data[ctarget] == 1)\n",
    "        good_dist = good / total_good if total_good != 0 else 0\n",
    "        bad_dist = bad / total_bad if total_bad != 0 else 0\n",
    "        \n",
    "        # 处理可能的零或NaN情况\n",
    "        if good_dist == 0:\n",
    "            good_dist = 1e-10\n",
    "        if bad_dist == 0:\n",
    "            bad_dist = 1e-10\n",
    "        \n",
    "        woe = np.log(good_dist / bad_dist)\n",
    "        woe_dict[bin] = woe\n",
    "        iv += (good_dist - bad_dist) * woe\n",
    "    \n",
    "    return woe_dict, iv\n",
    "# 特征选择和目标变量, 我们已经选择好了几个对目标值有影响力的变量所以只用表明目标是哪个，在这里就是“target”\n",
    "target_column = 'target'\n",
    "\n",
    "# 寻找最优分箱数量 和 WOE & IV 计算\n",
    "def select_best_binning(df, selected_features, target_column):\n",
    "    results = {}\n",
    "    woe_iv_values = {}\n",
    "    df_copies = {}  # 存储每次分箱后的数据框\n",
    "    for n_bins in range(2, 61):\n",
    "        df_copy = df.copy()\n",
    "        woe_iv_values[n_bins] = {}\n",
    "        for column in selected_features:\n",
    "            df_copy, binning_estimator = binning(df_copy, column, n_bins)\n",
    "            woe_dict, iv = calculate_woe_iv(df_copy, column + '_binned', target_column)\n",
    "            df_copy[column + '_woe'] = df_copy[column + '_binned'].map(woe_dict)\n",
    "            woe_iv_values[n_bins][column] = {'woe': woe_dict, 'iv': iv}\n",
    "        \n",
    "        # 模型训练和验证，用了逻辑回归模型\n",
    "        X = df_copy[[col + '_woe' for col in selected_features]]\n",
    "        y = df_copy[target_column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        results[n_bins] = auc\n",
    "        \n",
    "        df_copies[n_bins] = df_copy\n",
    "    \n",
    "    # 找到AUC最高的分箱方法,AUC代表Area Under the ROC Curve,用来评估分类模型性能的一个常用指标\n",
    "    best_n_bins = max(results, key=results.get)\n",
    "    best_df = df_copies[best_n_bins]\n",
    "    \n",
    "    return best_n_bins, results[best_n_bins], woe_iv_values[best_n_bins], best_df\n",
    "\n",
    "# 寻找和打印最后的选择数据,data为后面建立模型的数据库\n",
    "best_bins, best_auc, best_woe_iv_values, data  = select_best_binning(data, selected_features_final, target_column)\n",
    "\n",
    "# 打印结果\n",
    "print(\"\\n***************************\\n\"+f\"Best binning number: {best_bins} with AUC: {best_auc}\"+'\\n***************************\\n')\n",
    "#-----------------------------------------------------------------\n",
    "# 最终变量选择，选择IV值最高的前几个,但经过测试，发现按照我的步骤，11个变量会是最高准确率,在这里也就是省略了一个\n",
    "NumFea = 11\n",
    "iv_values_sorted = sorted(best_woe_iv_values.items(), key=lambda item: item[1]['iv'], reverse=True)\n",
    "top_iv_features = iv_values_sorted[:NumFea]\n",
    "\n",
    "# 打印出来为了更方便追踪数据\n",
    "print(\"\\nSelected top IV features for modeling:\")\n",
    "for feature, values in top_iv_features:\n",
    "    print(f\" {feature}, IV value: {values['iv']}\")\n",
    "# 提取特征名称列表\n",
    "top_iv_feature_names = [item[0] for item in top_iv_features]\n",
    "# print(\"\\nTop IV feature Selected Names:\", top_iv_feature_names)\n",
    "#-----------------------------------------------------------------\n",
    "# # 分割数据集并保存原始索引\n",
    "# 添加索引列\n",
    "data['Original_index'] = data.index + 2 \n",
    "# 分割数据集为训练集和测试集在我选择的变量中\n",
    "X = data[[col + '_woe' for col in top_iv_feature_names]]\n",
    "y = data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "test_indices = X_test.index\n",
    "# 确保模型训练使用了所有选择的特征\n",
    "print(f\"Features used in Model Training: {X_train.columns.tolist()}\")\n",
    "\n",
    "# 打印每个选择特征在分箱和WOE处理后的情况\n",
    "# for feature in selected_features_final:\n",
    "#     if feature + '_woe' not in data.columns:\n",
    "#         print(f\"Feature {feature}_woe is missing in the data\")\n",
    "#     else:\n",
    "#         print(f\"Feature {feature}_woe is present in the data\")\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "# 定义几个常用的模型,可以自行选择尝试---------------\n",
    "# 建立逻辑回归模型 (Logistic Regression)\n",
    "model_LR = LogisticRegression(max_iter=10000, C=1.0, penalty='l2', solver='lbfgs')\n",
    "\n",
    "# 建立决策树模型 (Decision Tree)\n",
    "model_DT = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 建立随机森林模型 (Random Forest)\n",
    "model_RF = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 建立梯度提升机模型 (Gradient Boosting Machine, GBM)\n",
    "model_GBM = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 建立支持向量机模型 (Support Vector Machine, SVM)\n",
    "model_SVC = SVC(probability=True, random_state=42)\n",
    "# 定义基础模型的元组\n",
    "estimators = [\n",
    "    ('lr', model_LR),\n",
    "    ('dt', model_DT),\n",
    "    ('rf', model_RF),\n",
    "    ('gbm', model_GBM),\n",
    "    ('svc', model_SVC)\n",
    "]\n",
    "\n",
    "# 定义堆叠分类器，最终模型选择了逻辑回归模型\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=model_LR\n",
    ")\n",
    "\n",
    "# 交叉验证评估模型性能\n",
    "models = {\n",
    "    'Logistic Regression': model_LR,\n",
    "    'Decision Tree': model_DT,\n",
    "    'Random Forest': model_RF,\n",
    "    'Gradient Boosting': model_GBM,\n",
    "    'Support Vector Machine': model_SVC,\n",
    "    'Stacking Classifier': stacking_clf\n",
    "}\n",
    "\n",
    "# 存储结果的字典\n",
    "results = {}\n",
    "\n",
    "# 训练和评估每个模型\n",
    "for name, model in models.items():\n",
    "    # 使用交叉验证评估模型性能\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"{name} Cross-validated accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'classification_report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# 找出准确率最高的模型\n",
    "best_model_name = max(results, key=lambda name: results[name]['accuracy'])\n",
    "best_model_results = results[best_model_name]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# 输出最佳模型的结果\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_model_results['accuracy']:.4f}\")\n",
    "print(f\"AUC: {best_model_results['auc']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(best_model_results['confusion_matrix'])\n",
    "print(\"Classification Report:\")\n",
    "print(best_model_results['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Score (A): 657.53\n",
      "Factor (B): 72.13\n",
      "Scorecard:\n",
      "Duration of Credit (month)_woe: weight=-0.8080, mean=0.4710, std=2.9962, score=19.4531\n",
      "Credit Amount_woe: weight=-0.7061, mean=-0.0925, std=2.1595, score=23.5849\n",
      "Account Balance_woe: weight=-0.7989, mean=0.1615, std=0.8732, score=65.9944\n",
      "Age (years)_woe: weight=-0.6500, mean=0.2507, std=1.9608, score=23.9124\n",
      "Payment Status of Previous Credit_woe: weight=-0.6623, mean=0.0344, std=0.5512, score=86.6647\n",
      "Value Savings/Stocks_woe: weight=-0.6856, mean=0.0629, std=0.4766, score=103.7727\n",
      "Purpose_woe: weight=-0.9993, mean=0.0459, std=0.4334, score=166.3108\n",
      "Most valuable available asset_woe: weight=-0.5296, mean=0.0247, std=0.3443, score=110.9610\n",
      "Length of current employment_woe: weight=-0.5641, mean=0.0217, std=0.2940, score=138.3820\n",
      "Concurrent Credits_woe: weight=-0.2772, mean=0.0147, std=0.2286, score=87.4429\n",
      "Sex & Marital Status_woe: weight=-1.1432, mean=0.0008, std=0.2129, score=387.3138\n",
      "\n",
      "Scorecard successfully saved to Outputs\\scorecard_Final.xlsx\n"
     ]
    }
   ],
   "source": [
    " # 构建评分卡函数\n",
    "import os\n",
    "def compute_scorecard(model, scaler, X, base_score=600, pdo=50):\n",
    "    coef = model.coef_[0]\n",
    "    intercept = model.intercept_[0]\n",
    "    features = X.columns\n",
    "\n",
    "    B = pdo / np.log(2)\n",
    "    A = base_score - B * intercept  # 这里包含了截距\n",
    "\n",
    "    scorecard = {}\n",
    "    for feature, weight in zip(features, coef):\n",
    "        feature_mean = scaler.mean_[list(features).index(feature)]\n",
    "        feature_std = scaler.scale_[list(features).index(feature)]\n",
    "        scorecard[feature] = {\n",
    "            'weight': weight,\n",
    "            'mean': feature_mean,\n",
    "            'std': feature_std,\n",
    "            'score': -weight * B / feature_std\n",
    "        }\n",
    "    return A, B, scorecard\n",
    "\n",
    "# 计算评分卡\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "# # 确认标准化后的数据形状和列名\n",
    "# print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "# print(f\"X_train_df columns: {X_train_df.columns.tolist()}\")\n",
    "# \n",
    "# print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "# print(f\"X_test_df columns: {X_test_df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "A, B, scorecard = compute_scorecard(best_model, scaler, X_train)\n",
    "# # 确认评分卡中的特征数量\n",
    "# print(f\"Number of features in scorecard: {len(scorecard)}\")\n",
    "# print(f\"Features in scorecard: {list(scorecard.keys())}\")\n",
    "\n",
    "# 打印评分卡\n",
    "print(f\"Base Score (A): {A:.2f}\")\n",
    "print(f\"Factor (B): {B:.2f}\")\n",
    "print(\"Scorecard:\")\n",
    "for feature, params in scorecard.items():\n",
    "    print(f\"{feature}: weight={params['weight']:.4f}, mean={params['mean']:.4f}, std={params['std']:.4f}, score={params['score']:.4f}\")\n",
    "\n",
    "# 将评分卡输出到Excel\n",
    "scorecard_df = pd.DataFrame(scorecard).T\n",
    "scorecard_df['feature'] = scorecard_df.index\n",
    "scorecard_df = scorecard_df[['feature', 'weight', 'mean', 'std', 'score']]\n",
    "output_dir = 'Outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file_path = os.path.join(output_dir, 'scorecard_Final.xlsx')\n",
    "scorecard_df.to_excel(output_file_path, index=False)\n",
    "print(f\"\\nScorecard successfully saved to {output_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:02:11.774386400Z",
     "start_time": "2024-08-08T06:02:11.735212500Z"
    }
   },
   "id": "ac35d1c9d1766272",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows have scores.\n",
      "Test Scores:\n",
      "0      456.962681\n",
      "1      953.947134\n",
      "2      -52.502541\n",
      "3      383.613406\n",
      "4      -89.175935\n",
      "          ...    \n",
      "395    951.990107\n",
      "396    433.915064\n",
      "397    682.437595\n",
      "398    378.589458\n",
      "399    678.453927\n",
      "Name: Duration of Credit (month)_woe, Length: 400, dtype: float64\n",
      "All rows have scores in the DataFrame.\n",
      "Results successfully saved to Outputs\\scoring_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 定义函数以计算评分卡分数并打印结果\n",
    "def calculate_score(X, A, B, scorecard):\n",
    "    scores = np.full(X.shape[0], A)\n",
    "    feature_scores = pd.DataFrame(index=X.index)\n",
    "    for feature, params in scorecard.items():\n",
    "        feature_contrib = (X[feature] - params['mean']) * params['score']\n",
    "        scores += feature_contrib\n",
    "        feature_scores[feature] = feature_contrib\n",
    "    return scores\n",
    "\n",
    "# 计算分数\n",
    "test_scores = calculate_score(X_test_df, A, B, scorecard)\n",
    "\n",
    "# 检查缺失分数的行\n",
    "if np.isnan(test_scores).any():\n",
    "    missing_indices = np.where(np.isnan(test_scores))[0]\n",
    "    print(\"Missing scores for the following indices:\")\n",
    "    print(missing_indices)\n",
    "else:\n",
    "    print(\"All rows have scores.\")\n",
    "\n",
    "# 打印所有分数\n",
    "print(\"Test Scores:\")\n",
    "print(test_scores)\n",
    "\n",
    "# 将测试集上的分数添加到原始测试数据中\n",
    "X_test_original = data.loc[test_indices].copy()\n",
    "X_test_original = X_test_original.reset_index(drop=True)  # 重置索引以确保顺序\n",
    "X_test_original['Credit Score'] = test_scores  # 直接按顺序添加分数\n",
    "\n",
    "# 检查缺失分数的行\n",
    "missing_scores = X_test_original[X_test_original['Credit Score'].isna()]\n",
    "if not missing_scores.empty:\n",
    "    print(\"Missing scores for the following rows:\")\n",
    "    print(missing_scores)\n",
    "else:\n",
    "    print(\"All rows have scores in the DataFrame.\")\n",
    "\n",
    "# 导出结果到Excel文件\n",
    "output_dir = 'Outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file_path = os.path.join(output_dir, 'scoring_results.xlsx')\n",
    "with pd.ExcelWriter(output_file_path) as writer:\n",
    "    X_test_original.to_excel(writer, sheet_name='Test Data with Scores', index=False)\n",
    "\n",
    "print(f\"Results successfully saved to {output_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T05:52:59.894073700Z",
     "start_time": "2024-08-08T05:52:59.425192400Z"
    }
   },
   "id": "e8002c542d2a44f4",
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
