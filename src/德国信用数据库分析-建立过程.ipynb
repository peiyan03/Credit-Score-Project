{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 德国信用卡数据与分析 — YTP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7695d3feff123c11"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Model Selection and Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:13.818345300Z",
     "start_time": "2024-08-08T06:01:13.471994700Z"
    }
   },
   "id": "e9b8b730a882b928",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 复制原文件建立新数据处理文件\n",
    "帮助我们确保有迹可循，分析过程中不会直接影响之前的数据库（损坏或者更改）\n",
    "可以在数据处理期间用备份的数据库进行一些修改改动等"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0b40250512179b9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully copied to resources/German_credit_NEW.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 读取源Excel文件\n",
    "source_file_path = 'resources/German_credit.xlsx' \n",
    "df = pd.read_excel(source_file_path)\n",
    "# 创建一个新的Excel文件并将数据写入其中\n",
    "new_file_path = 'resources/German_credit_NEW.xlsx'\n",
    "df.to_excel(new_file_path, index=False)\n",
    "print(f'Data successfully copied to {new_file_path}')\n",
    "# data现在是我们用来分析的源数据附件\n",
    "data = pd.read_excel(new_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:14.840378700Z",
     "start_time": "2024-08-08T06:01:13.822334900Z"
    }
   },
   "id": "38368c4f826547a5",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step1: 特征变量筛检（这里在分箱前）\n",
    "因为项目已经精简了数据库（预处理，分箱和编码都完成了），所以在此之上我选择了先进行变量分析去模拟下整个过程。目标是找到前几个比较有影响力的变量，后续再进行进一步分箱和模型建立。\n",
    "以下步骤是先画图去了解数据变量内容，和形式还有和目标变量的关系等。这样我们后面进行主要变量的选择会更明确，模型会更贴切我们的问题和目标。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "988dc9c3d8e8a53e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # 查看数据概况\n",
    "# print(df.info())\n",
    "# print(df.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:14.849264800Z",
     "start_time": "2024-08-08T06:01:14.843270Z"
    }
   },
   "id": "c7d46ff7daca8a56",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # 绘制数据直方图\n",
    "# def plot_histograms(df, columns):\n",
    "#     df[columns].hist(bins=20, figsize=(20, 15))\n",
    "#     plt.suptitle('Histogram of each class', fontsize= 20, fontweight='bold')\n",
    "#     plt.show()\n",
    "# plot_histograms(df, df.columns[:-1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:14.859875200Z",
     "start_time": "2024-08-08T06:01:14.847264400Z"
    }
   },
   "id": "d6104b47af6ef928",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # 绘制数据箱线图\n",
    "# def plot_boxplots(df, columns):\n",
    "#     df[columns].plot(kind='box', subplots=True, layout=(5, 5), sharex=False, sharey=False,figsize=(20, 15))\n",
    "#     plt.suptitle('Boxplot of features', fontsize= 20, fontweight='bold')\n",
    "#     plt.show()\n",
    "# plot_boxplots(df, df.columns[:-1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:14.891519200Z",
     "start_time": "2024-08-08T06:01:14.854222700Z"
    }
   },
   "id": "f14aad3b853b979e",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里我在数据库本有的变量中进行了对目标变量的影响进行比较，第一步是热图第二步是随机森林相关图。然后每个结果排序中拿前十名最后进行汇总。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da79accb8dbff540"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top correlated features with target: ['Account Balance', 'Payment Status of Previous Credit', 'Duration of Credit (month)', 'Value Savings/Stocks', 'Credit Amount', 'Most valuable available asset', 'Length of current employment', 'Concurrent Credits', 'Age (years)', 'Sex & Marital Status']\n"
     ]
    }
   ],
   "source": [
    "# 选择的影响力大的变量数量，前十名----------------------\n",
    "numVar = 10\n",
    "\n",
    "# 绘制数据相关性热图\n",
    "def plot_correlation_heatmap(df1):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    corr_matrix = df1.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap= 'coolwarm', cbar=True, square=True)\n",
    "    plt.title('Correlation Heatmap', fontsize= 20, fontweight='bold')\n",
    "    plt.show()\n",
    "# plot_correlation_heatmap(df)\n",
    "# 找到对target最有影响力的变量，依次排名，选numVar个较有影响力的\n",
    "def find_top_correlated_features(df2, target1, n):\n",
    "    corr_matrix = df2.corr()\n",
    "    target_cor = corr_matrix[target1].abs().sort_values(ascending=False)\n",
    "    top_features = target_cor.index[1:n+1]  # 排除掉target本身\n",
    "    return top_features, target_cor\n",
    "\n",
    "selected_features_CHMap, target_corr = find_top_correlated_features(df, 'target', numVar)\n",
    "print(f'Top correlated features with target: {selected_features_CHMap.tolist()}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:14.893514400Z",
     "start_time": "2024-08-08T06:01:14.865864600Z"
    }
   },
   "id": "6145b243dd804069",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top correlated features with target: ['Credit Amount', 'Account Balance', 'Age (years)', 'Duration of Credit (month)', 'Purpose', 'Payment Status of Previous Credit', 'Length of current employment', 'Value Savings/Stocks', 'Most valuable available asset', 'Instalment per cent']\n"
     ]
    }
   ],
   "source": [
    "# 绘制随机森林模型相关图\n",
    "# 特征选择和目标变量\n",
    "features = data.columns[data.columns != 'target'] \n",
    "target = 'target'\n",
    "# 分割数据集\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "# 构建随机森林模型以评估特征重要性\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "# 获取特征重要性\n",
    "importances = rf.feature_importances_\n",
    "feature_importances = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "# # 可视化特征重要性\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
    "# plt.xlabel('Importance')\n",
    "# plt.ylabel('Feature')\n",
    "# plt.title('Feature Importance')\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.show()\n",
    "# 筛选重要性较高的变量，依次排名，选numVar个较有影响力的\n",
    "selected_features_rf = feature_importances.nlargest(numVar, 'Importance')['Feature']\n",
    "print(f'Top correlated features with target: {selected_features_rf.tolist()}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:15.092355400Z",
     "start_time": "2024-08-08T06:01:14.876556500Z"
    }
   },
   "id": "7d6ab3079e3adbab",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sex & Marital Status', 'Account Balance', 'Concurrent Credits', 'Value Savings/Stocks', 'Payment Status of Previous Credit', 'Credit Amount', 'Most valuable available asset', 'Duration of Credit (month)', 'Length of current employment', 'Age (years)', 'Purpose', 'Instalment per cent']\n"
     ]
    }
   ],
   "source": [
    "# 采用了合并并去重采用了两列排名靠前的变量\n",
    "# 合并并去重\n",
    "combined_features = selected_features_rf.tolist() + selected_features_CHMap.tolist()\n",
    "unique_features_set = set(combined_features)\n",
    "selected_features_final = list(unique_features_set)\n",
    "print(selected_features_final)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:15.100735400Z",
     "start_time": "2024-08-08T06:01:15.094748700Z"
    }
   },
   "id": "fe80cb6c0f96b7fb",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: 数据分箱 （Binning）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df306bcf1cc01aff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过上一步数据筛选，我们会最终选择几个对目标变量有影响力的变量进行分箱，大大帮助后期模型的建立的准确性"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21b207e004538ecf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "binning（data, column, n_bins, strategy）:\n",
    "\n",
    "根据所选择的变量，分箱数，策略进行数据分箱\n",
    "- **n_bins**: 箱子数量\n",
    "\n",
    "- **strategy**: \n",
    "    - 'uniform': 每个箱宽度相等\n",
    "    - 'quantile': 按照等频分箱，每个箱中的数据点数量相同\n",
    "    - 'kmeans'：使用K-means分箱，根据数据点分布自动确定箱。更智能\n",
    "-----------------------------------------------------------\n",
    "calculate_woe_iv（data, column, target）:\n",
    "\n",
    "通过计算每个分箱中好样本和坏样本的比例，计算出每个分箱的WOE值，并根据这些WOE值计算总体的IV值。WOE值用于将分类变量转化为连续变量，而IV值用于衡量每个特征的预测能力。这样，你可以筛选出最有预测能力的特征，并将它们用于模型构建。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "263f3bef013186a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 分箱函数\n",
    "def binning(bdata, bcolumn, n_bins, strategy='uniform'): \n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "    bdata[bcolumn + '_binned'] = est.fit_transform(bdata[[bcolumn]])\n",
    "    return bdata, est\n",
    "\n",
    "# 计算WOE, IV函数\n",
    "def calculate_woe_iv(cdata, ccolumn, ctarget):\n",
    "    woe_dict = {}\n",
    "    iv = 0\n",
    "    total_good = sum(cdata[ctarget] == 0)\n",
    "    total_bad = sum(cdata[ctarget] == 1)\n",
    "    \n",
    "    for bin in sorted(cdata[ccolumn].unique()):\n",
    "        bin_data = cdata[cdata[ccolumn] == bin]\n",
    "        good = sum(bin_data[ctarget] == 0)\n",
    "        bad = sum(bin_data[ctarget] == 1)\n",
    "        good_dist = good / total_good if total_good != 0 else 0\n",
    "        bad_dist = bad / total_bad if total_bad != 0 else 0\n",
    "        \n",
    "        # 处理可能的零或NaN情况\n",
    "        if good_dist == 0:\n",
    "            good_dist = 1e-10\n",
    "        if bad_dist == 0:\n",
    "            bad_dist = 1e-10\n",
    "        \n",
    "        woe = np.log(good_dist / bad_dist)\n",
    "        woe_dict[bin] = woe\n",
    "        iv += (good_dist - bad_dist) * woe\n",
    "    \n",
    "    return woe_dict, iv\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:15.113004700Z",
     "start_time": "2024-08-08T06:01:15.102142500Z"
    }
   },
   "id": "bfbdef47cb1e38e0",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC scores for different binning numbers: {2: 0.7240476190476188, 3: 0.7287499999999999, 4: 0.7429761904761906, 5: 0.7611904761904762, 6: 0.7613095238095238, 7: 0.7616666666666666, 8: 0.758095238095238, 9: 0.7592857142857143, 10: 0.7739285714285714, 11: 0.7646428571428571, 12: 0.7707142857142858, 13: 0.7597619047619047, 14: 0.7673809523809524, 15: 0.784047619047619, 16: 0.7708333333333334, 17: 0.7798809523809525, 18: 0.7736904761904762, 19: 0.7792857142857142, 20: 0.7813095238095238, 21: 0.785952380952381, 22: 0.7790476190476191, 23: 0.7825, 24: 0.7846428571428571, 25: 0.7828571428571428, 26: 0.7858333333333333, 27: 0.7838095238095238, 28: 0.7804761904761904, 29: 0.7822619047619047, 30: 0.7852380952380952, 31: 0.788452380952381, 32: 0.7841666666666666, 33: 0.7892857142857143, 34: 0.8053571428571429, 35: 0.7942857142857143, 36: 0.8059523809523809, 37: 0.794047619047619, 38: 0.8029761904761905, 39: 0.8014285714285715, 40: 0.8076190476190477, 41: 0.7867857142857143, 42: 0.7807142857142858, 43: 0.7970238095238095, 44: 0.791547619047619, 45: 0.8096428571428572, 46: 0.8057142857142857, 47: 0.7936904761904763, 48: 0.7976190476190477, 49: 0.7995238095238095, 50: 0.8008333333333333, 51: 0.8092857142857144, 52: 0.8150000000000001, 53: 0.7998809523809525, 54: 0.8203571428571428, 55: 0.8080952380952381, 56: 0.8045238095238095, 57: 0.8122619047619047, 58: 0.8011904761904762, 59: 0.8080952380952381, 60: 0.8138095238095238}\n",
      "\n",
      "***************************\n",
      "Best binning number: 54 with AUC: 0.8203571428571428\n",
      "***************************\n"
     ]
    }
   ],
   "source": [
    "# 特征选择和目标变量, 我们已经选择好了几个对目标值有影响力的变量所以只用表明目标是哪个，在这里就是“target”\n",
    "target_column = 'target'\n",
    "\n",
    "# 寻找最优分箱数量 和 WOE & IV 计算\n",
    "def select_best_binning(df, selected_features, target_column):\n",
    "    results = {}\n",
    "    woe_iv_values = {}\n",
    "    df_copies = {}  # 存储每次分箱后的数据框\n",
    "    for n_bins in range(2, 61):\n",
    "        df_copy = df.copy()\n",
    "        woe_iv_values[n_bins] = {}\n",
    "        for column in selected_features:\n",
    "            df_copy, binning_estimator = binning(df_copy, column, n_bins)\n",
    "            woe_dict, iv = calculate_woe_iv(df_copy, column + '_binned', target_column)\n",
    "            df_copy[column + '_woe'] = df_copy[column + '_binned'].map(woe_dict)\n",
    "            woe_iv_values[n_bins][column] = {'woe': woe_dict, 'iv': iv}\n",
    "        \n",
    "        # 模型训练和验证，用了逻辑回归模型\n",
    "        X = df_copy[[col + '_woe' for col in selected_features]]\n",
    "        y = df_copy[target_column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        results[n_bins] = auc\n",
    "        \n",
    "        df_copies[n_bins] = df_copy\n",
    "    \n",
    "    # 找到AUC最高的分箱方法,AUC代表Area Under the ROC Curve,用来评估分类模型性能的一个常用指标\n",
    "    best_n_bins = max(results, key=results.get)\n",
    "    best_df = df_copies[best_n_bins]\n",
    "    \n",
    "    print(\"AUC scores for different binning numbers:\", results)\n",
    "    return best_n_bins, results[best_n_bins], woe_iv_values[best_n_bins], best_df\n",
    "\n",
    "# 寻找和打印最后的选择数据,data为后面建立模型的数据库\n",
    "best_bins, best_auc, best_woe_iv_values, data  = select_best_binning(data, selected_features_final, target_column)\n",
    "\n",
    "# 打印结果\n",
    "print(\"\\n***************************\\n\"+f\"Best binning number: {best_bins} with AUC: {best_auc}\"+'\\n***************************\\n')\n",
    "\n",
    "# for column, values in best_woe_iv_values.items():\n",
    "#     print(f\"Feature: {column}\")\n",
    "#     print(f\"WOE values: {values['woe']}\")\n",
    "#     print(f\"IV value: {values['iv']}\")\n",
    "#     print(\"---------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:21.433028300Z",
     "start_time": "2024-08-08T06:01:15.113004700Z"
    }
   },
   "id": "2a27f9268516be62",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里可以看到结果显示分箱数为54的时候，AUC值是最高的，所以我选择采用这个分箱数量并计算WOE和IV值用于后面的模型训练。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e5e967dd1e5c071"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: 模型构造与验证\n",
    "\n",
    "经过分箱分析和选择后，我们可以用分箱后的数据进行模型构造"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68d46b8cdb0c794a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected top IV features for modeling:\n",
      " Duration of Credit (month), IV value: 1.0495353305806965\n",
      " Credit Amount, IV value: 1.0314557255592238\n",
      " Account Balance, IV value: 0.6660115033513336\n",
      " Age (years), IV value: 0.6129095040344794\n",
      " Payment Status of Previous Credit, IV value: 0.2932335473908263\n",
      " Value Savings/Stocks, IV value: 0.19600955690422672\n",
      " Purpose, IV value: 0.16919506567307832\n",
      " Most valuable available asset, IV value: 0.11263826240979674\n",
      " Length of current employment, IV value: 0.086433631026641\n",
      " Concurrent Credits, IV value: 0.057614541955647885\n",
      " Sex & Marital Status, IV value: 0.04467067763379073\n",
      "\n",
      "Top IV feature Selected Names: ['Duration of Credit (month)', 'Credit Amount', 'Account Balance', 'Age (years)', 'Payment Status of Previous Credit', 'Value Savings/Stocks', 'Purpose', 'Most valuable available asset', 'Length of current employment', 'Concurrent Credits', 'Sex & Marital Status']\n"
     ]
    }
   ],
   "source": [
    "# 最终变量选择，选择IV值最高的前几个,但经过测试，发现按照我的步骤，11个变量会是最高准确率\n",
    "NumFea = 11\n",
    "iv_values_sorted = sorted(best_woe_iv_values.items(), key=lambda item: item[1]['iv'], reverse=True)\n",
    "top_iv_features = iv_values_sorted[:NumFea]\n",
    "\n",
    "# 打印出来为了更方便追踪数据\n",
    "print(\"\\nSelected top IV features for modeling:\")\n",
    "for feature, values in top_iv_features:\n",
    "    print(f\" {feature}, IV value: {values['iv']}\")\n",
    "# 提取特征名称列表\n",
    "top_iv_feature_names = [item[0] for item in top_iv_features]\n",
    "print(\"\\nTop IV feature Selected Names:\", top_iv_feature_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:21.441829400Z",
     "start_time": "2024-08-08T06:01:21.436838400Z"
    }
   },
   "id": "8f00b13c4a121ba",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    " **train_test_split():**\n",
    "  - arrays：需要分割的输入数据。通常是特征矩阵和目标向量。\n",
    "  - test_size：测试集所占的比例，可以是浮点数（表示比例）或整数（表示测试集样本数）。默认值为0.25。\n",
    "  - train_size：训练集所占的比例，可以是浮点数（表示比例）或整数（表示训练集样本数）。如果未指定，默认值为 1 - test_size。\n",
    "  - random_state：随机种子，确保每次运行都得到相同的分割结果。可以是整数或 None。\n",
    "  - shuffle：是否在分割之前对数据进行打乱。默认值为 True。\n",
    "  - stratify：按某个变量分层抽样，确保分层变量在训练集和测试集中有相同的分布。通常用于分类任务。 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "365e2872d5acd1cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 分割数据为训练集和测试机\n",
    "我选择的比例为 train : test = 6 : 4\n",
    "当然这个数值也会影响到最后的结果，这个比例对我的步骤来讲会产生较高的准确率。但是如果将训练集进一步减少导致测试集数量大于它的话，我担心模型最后的局限性所以通常训练集是占比较大的。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe9d96f0859cb76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 分割数据集为训练集和测试集在我选择的变量中\n",
    "X = data[[col + '_woe' for col in top_iv_feature_names]]\n",
    "y = data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "# print('y_train:', y_train)\n",
    "# print('y_test:', y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:21.452890700Z",
     "start_time": "2024-08-08T06:01:21.441829400Z"
    }
   },
   "id": "555222581e831201",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 主流模型基础运用\n",
    "\n",
    "这里主要是演示各个主流模型基础运用，我这里直接运用模型并直接找出准确率最高的模型去更直观的比较。当然也可以一个个去尝试并记录结果。\n",
    "这一步没有添加任何额外的步骤去比较，只是简单的用模型去训练并预测，然后最后准确率对比选择最好的。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d48b8366b43c4c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 定义几个常用的模型,可以自行选择尝试---------------\n",
    "# 建立逻辑回归模型 (Logistic Regression)\n",
    "model_LR = LogisticRegression(max_iter=10000, C=1.0, penalty='l2', solver='lbfgs')\n",
    "\n",
    "# 建立决策树模型 (Decision Tree)\n",
    "model_DT = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 建立随机森林模型 (Random Forest)\n",
    "model_RF = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 建立梯度提升机模型 (Gradient Boosting Machine, GBM)\n",
    "model_GBM = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 建立支持向量机模型 (Support Vector Machine, SVM)\n",
    "model_SVC = SVC(probability=True, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:21.461673600Z",
     "start_time": "2024-08-08T06:01:21.452890700Z"
    }
   },
   "id": "aafccd7227efc851",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Logistic Regression\n",
      "Accuracy: 0.8075\n",
      "AUC: 0.8439583333333334\n",
      "Confusion Matrix:\n",
      "[[248  32]\n",
      " [ 45  75]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       280\n",
      "           1       0.70      0.62      0.66       120\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.77      0.76      0.76       400\n",
      "weighted avg       0.80      0.81      0.80       400\n"
     ]
    }
   ],
   "source": [
    "# 对数据进行标准化处理,标准化是指将数据调整到均值为0、标准差为1的分布。\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 定义模型\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=10000, C=1.0, penalty='l2', solver='lbfgs'),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# 存储结果的字典\n",
    "results = {}\n",
    "# 训练和评估每个模型\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'classification_report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# 找出准确率最高的模型\n",
    "best_model_name = max(results, key=lambda name: results[name]['accuracy'])\n",
    "best_model_results = results[best_model_name]\n",
    "\n",
    "# 输出最佳模型的结果\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_model_results['accuracy']}\")\n",
    "print(f\"AUC: {best_model_results['auc']}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(best_model_results['confusion_matrix'])\n",
    "print(\"Classification Report:\")\n",
    "print(best_model_results['classification_report'])\n",
    "\n",
    "#\n",
    "# # 这是最基础的一个模型下的运算代码, 如果想要一个个去查验,可以直接用下面注释代码\n",
    "# # 记得model后缀加上对应的代码!!!\n",
    "# # 在训练集上训练模型\n",
    "# model.fit(X_train, y_train)\n",
    "# # 在测试集上进行预测\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "# \n",
    "# # 评估模型性能\n",
    "# print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "# print(f'AUC: {roc_auc_score(y_test, y_pred_proba)}')\n",
    "# print('Confusion Matrix:')\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:21.898954500Z",
     "start_time": "2024-08-08T06:01:21.457683400Z"
    }
   },
   "id": "5c620204294d5172",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 提升模型准确率\n",
    "\n",
    "通常，为了提高准确率，在模型建立和训练时应加上额外的步骤，比如交叉评估等。\n",
    "我这里运用了集成学习，交叉验证，其实就是上一段代码的提升。还是对主流模型进行评估比较然后选择最好的模型，这样会更严谨而且我们能看到模型的数据。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7e11927948e80bf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross-validated accuracy: 0.7917 ± 0.0376\n",
      "Decision Tree Cross-validated accuracy: 0.6900 ± 0.0318\n",
      "Random Forest Cross-validated accuracy: 0.7533 ± 0.0476\n",
      "Gradient Boosting Cross-validated accuracy: 0.7500 ± 0.0253\n",
      "Support Vector Machine Cross-validated accuracy: 0.7650 ± 0.0403\n",
      "Voting Classifier Cross-validated accuracy: 0.7683 ± 0.0232\n",
      "\n",
      "Best Model: Logistic Regression\n",
      "Accuracy: 0.8075\n",
      "AUC: 0.8440\n",
      "Confusion Matrix:\n",
      "[[248  32]\n",
      " [ 45  75]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       280\n",
      "           1       0.70      0.62      0.66       120\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.77      0.76      0.76       400\n",
      "weighted avg       0.80      0.81      0.80       400\n"
     ]
    }
   ],
   "source": [
    "# 集成学习 - 投票分类器\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', model_LR),\n",
    "        ('dt', model_DT),\n",
    "        ('rf', model_RF),\n",
    "        ('gbm', model_GBM),\n",
    "        ('svc', model_SVC)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# 交叉验证评估模型性能\n",
    "models = {\n",
    "    'Logistic Regression': model_LR,\n",
    "    'Decision Tree': model_DT,\n",
    "    'Random Forest': model_RF,\n",
    "    'Gradient Boosting': model_GBM,\n",
    "    'Support Vector Machine': model_SVC,\n",
    "    'Voting Classifier': voting_clf\n",
    "}\n",
    "\n",
    "# 存储结果的字典\n",
    "results = {}\n",
    "\n",
    "# 训练和评估每个模型\n",
    "for name, model in models.items():\n",
    "    # 使用交叉验证评估模型性能\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"{name} Cross-validated accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'classification_report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# 找出准确率最高的模型\n",
    "best_model_name = max(results, key=lambda name: results[name]['accuracy'])\n",
    "best_model_results = results[best_model_name]\n",
    "\n",
    "# 输出最佳模型的结果\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_model_results['accuracy']:.4f}\")\n",
    "print(f\"AUC: {best_model_results['auc']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(best_model_results['confusion_matrix'])\n",
    "print(\"Classification Report:\")\n",
    "print(best_model_results['classification_report'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:25.907636500Z",
     "start_time": "2024-08-08T06:01:21.906934900Z"
    }
   },
   "id": "bed04ea2b57f139d",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "由此可见，两次结果都是显示逻辑回归模型为最好的，结果也是一样的因为训练集和测试集，还有其它参数都没有变化。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ae438c9d104ad40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 模型堆叠 （Stacking）\n",
    "我们可以通过再一次的训练集成学习模型进一步提高模型的性能。\n",
    "即将不同模型的预测结果作为新的特征输入到一个最终模型中进行训练。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "328eb16502fc7829"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 第二次运用逻辑回归模型去训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3aa9b70e238cd257"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Logistic Regression Model\n",
      "Accuracy: 0.6725\n",
      "AUC: 0.7781\n",
      "Confusion Matrix:\n",
      "[[214  66]\n",
      " [ 65  55]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77       280\n",
      "           1       0.45      0.46      0.46       120\n",
      "\n",
      "    accuracy                           0.67       400\n",
      "   macro avg       0.61      0.61      0.61       400\n",
      "weighted avg       0.67      0.67      0.67       400\n"
     ]
    }
   ],
   "source": [
    "# 使用最佳模型的预测结果作为新的特征\n",
    "X_train_meta = np.column_stack([model.predict_proba(X_train)[:, 1] for model in models.values()])\n",
    "X_test_meta = np.column_stack([model.predict_proba(X_test)[:, 1] for model in models.values()])\n",
    "\n",
    "# 使用新的特征训练逻辑回归模型\n",
    "meta_model_LR = LogisticRegression(max_iter=10000, C=1.0, penalty='l2', solver='lbfgs')\n",
    "meta_model_LR.fit(X_train_meta, y_train)\n",
    "\n",
    "# 进行预测\n",
    "y_pred_meta = meta_model_LR.predict(X_test_meta)\n",
    "y_pred_proba_meta = meta_model_LR.predict_proba(X_test_meta)[:, 1]\n",
    "\n",
    "# 评估最终逻辑回归模型性能\n",
    "accuracy_meta = accuracy_score(y_test, y_pred_meta)\n",
    "auc_meta = roc_auc_score(y_test, y_pred_proba_meta)\n",
    "\n",
    "# 输出最终逻辑回归模型的结果\n",
    "print(\"\\nFinal Logistic Regression Model\")\n",
    "print(f\"Accuracy: {accuracy_meta:.4f}\")\n",
    "print(f\"AUC: {auc_meta:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_meta))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_meta))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:26.049856Z",
     "start_time": "2024-08-08T06:01:25.913620100Z"
    }
   },
   "id": "27df5135d3843155",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "但是我们可以看到这么做并不会在这里提高模型的准确率，所以说明可能是数据大小还有之前步骤导致的。\n",
    "还有另外一种办法是套用 StackingClassifier,这个是更加正式的堆叠方法，将他添加到我们之前的模型里，这样这个模型中还会包含一个堆叠模型去进行训练\n",
    "- StackingClassifier是Scikit-Learn中实现堆叠模型的一个类。它使用多个基础模型（第一层模型）来生成预测，然后将这些预测作为输入特征，传递给一个最终的模型（第二层模型）进行最终预测。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49dc966ab50b0f19"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross-validated accuracy: 0.7917 ± 0.0376\n",
      "Decision Tree Cross-validated accuracy: 0.6900 ± 0.0318\n",
      "Random Forest Cross-validated accuracy: 0.7533 ± 0.0476\n",
      "Gradient Boosting Cross-validated accuracy: 0.7500 ± 0.0253\n",
      "Support Vector Machine Cross-validated accuracy: 0.7650 ± 0.0403\n",
      "Stacking Classifier Cross-validated accuracy: 0.7850 ± 0.0305\n",
      "\n",
      "Best Model: Stacking Classifier\n",
      "Accuracy: 0.8100\n",
      "AUC: 0.8420\n",
      "Confusion Matrix:\n",
      "[[253  27]\n",
      " [ 49  71]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87       280\n",
      "           1       0.72      0.59      0.65       120\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.78      0.75      0.76       400\n",
      "weighted avg       0.80      0.81      0.80       400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义基础模型的元组\n",
    "estimators = [\n",
    "    ('lr', model_LR),\n",
    "    ('dt', model_DT),\n",
    "    ('rf', model_RF),\n",
    "    ('gbm', model_GBM),\n",
    "    ('svc', model_SVC)\n",
    "]\n",
    "\n",
    "# 定义堆叠分类器，最终模型选择了逻辑回归模型\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(max_iter=10000)\n",
    ")\n",
    "\n",
    "# 交叉验证评估模型性能\n",
    "models = {\n",
    "    'Logistic Regression': model_LR,\n",
    "    'Decision Tree': model_DT,\n",
    "    'Random Forest': model_RF,\n",
    "    'Gradient Boosting': model_GBM,\n",
    "    'Support Vector Machine': model_SVC,\n",
    "    'Stacking Classifier': stacking_clf\n",
    "}\n",
    "\n",
    "# 存储结果的字典\n",
    "results = {}\n",
    "\n",
    "# 训练和评估每个模型\n",
    "for name, model in models.items():\n",
    "    # 使用交叉验证评估模型性能\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"{name} Cross-validated accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # 在训练集上训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'classification_report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# 找出准确率最高的模型\n",
    "best_model_name = max(results, key=lambda name: results[name]['accuracy'])\n",
    "best_model_results = results[best_model_name]\n",
    "\n",
    "\n",
    "# 输出最佳模型的结果\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_model_results['accuracy']:.4f}\")\n",
    "print(f\"AUC: {best_model_results['auc']:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(best_model_results['confusion_matrix'])\n",
    "print(\"Classification Report:\")\n",
    "print(best_model_results['classification_report'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:01:38.476524700Z",
     "start_time": "2024-08-08T06:01:26.052849Z"
    }
   },
   "id": "2c756841f5ae4b43",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们可以明显看到准确率有提升，但是并不明显。并且相比之前逻辑回归模型，坏客户（1）的recall率还是下降了点，这也并不是我们想看到的。\n",
    "这一方法在本数据库中只有1k个数据可能提升不大，但是的确是个很好用的办法。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bb509685acb976a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: 评分卡和信用评分模型建立\n",
    "\n",
    "最后，在我们模型建立并且训练完成以后，我们可以开始构建评分卡和最后的评分模型建立。经过上一步的模型验证，我们确认了最后会运用逻辑回归模型去建立我们的评分系统。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b143087437e44807"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Score (A): 600.00\n",
      "Factor (B): 72.13\n",
      "Scorecard:\n",
      "Duration of Credit (month)_woe: weight=-1.8856, mean=0.4710, std=2.9962, score=45.3955\n",
      "Credit Amount_woe: weight=-1.3579, mean=-0.0925, std=2.1595, score=45.3568\n",
      "Account Balance_woe: weight=-0.6946, mean=0.1615, std=0.8732, score=57.3833\n",
      "Age (years)_woe: weight=-1.1180, mean=0.2507, std=1.9608, score=41.1306\n",
      "Payment Status of Previous Credit_woe: weight=-0.3779, mean=0.0344, std=0.5512, score=49.4547\n",
      "Value Savings/Stocks_woe: weight=-0.3414, mean=0.0629, std=0.4766, score=51.6781\n",
      "Purpose_woe: weight=-0.4399, mean=0.0459, std=0.4334, score=73.2134\n",
      "Most valuable available asset_woe: weight=-0.2174, mean=0.0247, std=0.3443, score=45.5424\n",
      "Length of current employment_woe: weight=-0.1693, mean=0.0217, std=0.2940, score=41.5371\n",
      "Concurrent Credits_woe: weight=-0.0735, mean=0.0147, std=0.2286, score=23.1803\n",
      "Sex & Marital Status_woe: weight=-0.2936, mean=0.0008, std=0.2129, score=99.4727\n"
     ]
    }
   ],
   "source": [
    "best_model = model_LR #最后这里我还是选择了逻辑回归模型\n",
    "\n",
    "# 为了确保之前的步骤不会影响评分卡建立测试，这里再一次分组\n",
    "X = data[[col + '_woe' for col in top_iv_feature_names]]\n",
    "y = data[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# 构建评分卡\n",
    "def compute_scorecard(model, scaler, X, base_score=600, pdo=50):\n",
    "    # 获取模型系数和特征\n",
    "    coef = model.coef_[0]\n",
    "    intercept = model.intercept_[0]\n",
    "    features = X.columns\n",
    "\n",
    "    # 计算B参数\n",
    "    B = pdo / np.log(2)\n",
    "    \n",
    "    # 设置基础分数\n",
    "    A = base_score\n",
    "\n",
    "    # 创建评分卡\n",
    "    scorecard = {}\n",
    "    for feature, weight in zip(features, coef):\n",
    "        feature_mean = scaler.mean_[list(features).index(feature)]\n",
    "        feature_std = scaler.scale_[list(features).index(feature)]\n",
    "        scorecard[feature] = {\n",
    "            'weight': weight,\n",
    "            'mean': feature_mean,\n",
    "            'std': feature_std,\n",
    "            'score': -weight * B / feature_std\n",
    "        }\n",
    "    return A, B, scorecard\n",
    "# 缩放数据\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 将 X_train_scaled 和 X_test_scaled 转换为 DataFrame\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "# 计算评分卡\n",
    "A, B, scorecard = compute_scorecard(best_model, scaler, X_train)\n",
    "\n",
    "# 打印评分卡\n",
    "print(f\"Base Score (A): {A:.2f}\")\n",
    "print(f\"Factor (B): {B:.2f}\")\n",
    "print(\"Scorecard:\")\n",
    "for feature, params in scorecard.items():\n",
    "    print(f\"{feature}: weight={params['weight']:.4f}, mean={params['mean']:.4f}, std={params['std']:.4f}, score={params['score']:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:06:56.891932800Z",
     "start_time": "2024-08-08T06:06:56.859643400Z"
    }
   },
   "id": "2f3b715032f6898b",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows have scores.\n",
      "Test Scores:\n",
      "0      498.820209\n",
      "1      612.985437\n",
      "2      312.506327\n",
      "3      490.985661\n",
      "4      337.170807\n",
      "          ...    \n",
      "395    697.009871\n",
      "396    447.242637\n",
      "397    552.053661\n",
      "398    649.050514\n",
      "399    511.918554\n",
      "Name: Duration of Credit (month)_woe, Length: 400, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 定义函数以计算评分卡分数并打印结果\n",
    "def calculate_score(X, A, B, scorecard):\n",
    "    scores = np.full(X.shape[0], A)\n",
    "    feature_scores = pd.DataFrame(index=X.index)\n",
    "    for feature, params in scorecard.items():\n",
    "        feature_contrib = (X[feature] - params['mean']) * params['score']\n",
    "        scores += feature_contrib\n",
    "        feature_scores[feature] = feature_contrib\n",
    "    return scores\n",
    "\n",
    "# 计算分数\n",
    "test_scores = calculate_score(X_test_df, A, B, scorecard)\n",
    "\n",
    "# 检查缺失分数的行\n",
    "if np.isnan(test_scores).any():\n",
    "    missing_indices = np.where(np.isnan(test_scores))[0]\n",
    "    print(\"Missing scores for the following indices:\")\n",
    "    print(missing_indices)\n",
    "else:\n",
    "    print(\"All rows have scores.\")\n",
    "\n",
    "# 打印所有分数\n",
    "print(\"Test Scores:\")\n",
    "print(test_scores)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T06:08:16.665372600Z",
     "start_time": "2024-08-08T06:08:16.637941600Z"
    }
   },
   "id": "be7df4b2da00578",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "最后这里打印出来所有测试集的分数，也能看到Length 是400 所以每个测试集都应该有自己对应的分数。\n",
    "至此我们完成了所有基本的步骤。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a564381073a0389"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
